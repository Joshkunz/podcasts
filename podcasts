#!/usr/bin/python
from operator import methodcaller
from collections import defaultdict
import re
import fnmatch
import os
import codecs
import yaml
import sys
import requests
from lxml import etree
from datetime import datetime
import mimetypes
import argparse
from pprint import pprint

## Configuration
options = dict(
    root_dir = "~/podcasts",
    feed_store = "feeds",
    episode_store = "episodes"
)

# check for a config file in ~/.podcastrc
rc_path = os.path.join(os.environ["HOME"], '.podcastrc')
if os.path.exists(rc_path):
    execfile(rc_path, options)

# All paths are relative to this one
ROOT_DIR = options["root_dir"]

# Where to keep the cached feeds
FEED_STORE = options["feed_store"]

# Where to store downloaded episodes
EPISODE_STORE = options["episode_store"] 

## Constants

UTF8_WRITER = codecs.getwriter("utf-8")
UTF8_READER = codecs.getreader("utf-8")
FEED_ENDINGS = ('atom', 'rss', 'xml')
FEED_TIME = "%a, %d %b %Y %H:%M:%S"

ROOT_DIR = os.path.expanduser(ROOT_DIR)
ROOT_DIR = os.path.expandvars(ROOT_DIR)
os.chdir(ROOT_DIR)

sys.stdout = UTF8_WRITER(sys.stdout)

## Classes

class SimpleProgress(object):
    "Simple progress printer"

    def __init__(self, round_to=2):
        self.round_to = round_to

    def starting_writing(self):
        "called on the start of a file download"
        pass

    def chunk_written(self, read, total):
        "called on the write for every chunk"
        if total is not None:
            percent_complete = round((float(read)/total)*100, self.round_to)
            sys.stdout.write("\rProgress: {0}%".format(percent_complete))
        else:
            sys.stdout.write("\rDownloaded: {0} bytes".format(read))
        sys.stdout.flush()

    def done_writing(self):
        "called when writing is finished"
        sys.stdout.write("\n")
        sys.stdout.flush()

## Utilities/General

def serialize_to_unicode(x):
    "serialize an item to unicode"
    if isinstance(x, list):
        return _s2u(x, xrange(len(x)))
    elif isinstance(x, dict):
        return _s2u(x, x.keys())
    else:
        return unicode(x)

def _s2u(x, key):
    "mutable serialization helper"
    for index in key:
        x[index] = serialize_to_unicode(x[index])
    return x

def get_safe(list, index, default=None):
    "get a list index with a default value"
    try:
        return list[index]
    except IndexError:
        return default

def split_at(x, n):
    "Split x at position n"
    return [x[:n], x[n:]]

## Utilities/Paths

def ensure_cast_dir(cast_name):
    "Make sure the podcast's episode folder exists."
    path = os.path.join(EPISODE_STORE, cast_name)
    if os.path.exists(path) != True:
        os.makedirs(path)
    return path

def ensure_feed_dir():
    "Ensure the location that stores the feeds exists"
    if os.path.exists(FEED_STORE) != True:
        os.makedirs(FEED_STORE)

def feed_pathname(feed_name):
    "generate the name for a feed cache"
    return os.path.join(FEED_STORE, ".".join((feed_name, "rss")))

## Utilities/Iterators

def iter_feeds(feedfile="feeds.yaml"):
    "iterate through a the feed definition file"
    with open(feedfile) as feed_file:
        feeds = yaml.load(feed_file)
        for name, url in feeds.iteritems():
            feed_d = dict(
                name = name,
                url = url)
            yield feed_d

def iter_feed_mask(iterable, only=[]):
    "Mask the iterator to only give requested feeds."
    for x in iterable:
        if x["name"] in only:
            yield x
        else:
            pass

def iter_details(parsed_feed):
    "iterate over items with parsed details"
    for item in parsed_feed.xpath("//item"):
        try:
            yield item_details(item)
        except IndexError:
            # Some critical value couldn't be found...
            pass

def feed_iterator(only=[]):
    "iterate over feeds, with specified 'only'"
    if only == []:
        return iter_feeds()
    else:
        return iter_feed_mask(iter_feeds(), only)

## Utilities/Parsing

def get_extension(headers, filed):
    "Get the file extension for the file"
    if headers["Content-Type"] is not None:
        return mimetypes.guess_extension(headers["Content-Type"])
    elif "type" in filed:
        return mimetypes.guess_extension(filed["type"])
    else: 
        return ".unknown"

def find_file(feed_item):
    "get podcast file attributes"
    enclosure = feed_item.xpath("enclosure")[0]
    return dict(enclosure.attrib)

def parse_date(date_string):
    "Parse a date from an RSS feed into a datime object"
    date, offset = map(methodcaller("strip"), date_string.rsplit(" ", 1))
    date = datetime.strptime(date, FEED_TIME)
    return date

def item_details(feed_item):
    "get a dictonary containing each feed item's data as a dictonary"
    details = dict({
        "title": feed_item.xpath("title/text()")[0],
        "description": feed_item.xpath("description/text()")[0],
        "link": get_safe(feed_item.xpath("link/text()"), 0, ""),
        "published": parse_date(feed_item.xpath("pubDate/text()")[0]),
        "file": find_file(feed_item)
    })
    return serialize_to_unicode(details)

## Utilities/Printers

def get_length(headers, filed):
    "try and figure out the length of a file"
    if headers["Content-Length"] is not None:
        return int(headers["Content-Length"])
    elif "length" in filed and int(filed["length"]) != 0:
        return int(filed["length"])
    else:
        return None

def progressive_read_url(page, out_file, length, decode_unicode=False,
                         chunk_size=1024, print_hook=SimpleProgress()):
    """Read a page from the internet in chunks, 
    print_hook can be used to display a progress message"""

    read_bytes = 0
    print_hook.starting_writing()
    for chunk in page.iter_content(chunk_size=chunk_size, 
                                   decode_unicode=decode_unicode):
        out_file.write(chunk)
        read_bytes += chunk_size
        print_hook.chunk_written(read_bytes, length) 
    print_hook.done_writing() #tell the printer reading is Done

# Update Functionality

def update_feedfile(url, name):
    "Get a feed from the internet"
    ensure_feed_dir() # sanity check
    print "Checking: {0}".format(name)
    page = requests.get(url)
    with open(feed_pathname(name), 'w') as feed_file:
        writer = UTF8_WRITER(feed_file)
        page.encoding = "utf-8"
        progressive_read_url(page, writer, 
                             int(page.headers["Content-Length"]) if \
                                page.headers["Content-Length"] is not None 
                                else None,
                             decode_unicode=True)

## Download Functionality

def download_podcast(cast_name, item_details):
    "Download a podcast item"
    path = ensure_cast_dir(cast_name)

    # Get the page and file attributes
    page = requests.get(item_details["file"]["url"])
    length, ext = get_length(page.headers, item_details["file"]), \
                  get_extension(page.headers, item_details["file"])

    # Download the file
    path = os.path.join(path, item_details["title"]+ext)
    print "Downloading:", item_details["title"]
    with open(path, 'wb') as podcast_file:
        progressive_read_url(page, podcast_file, length)

def update(args):
    "Update feeds."
    for feed in feed_iterator(args.podcast):
        try:
            if args.streaming is True:
                update_feedfile_streaming(feed["url"], feed["name"])
            else:
                update_feedfile(feed["url"], feed["name"])
        except Exception, e:
            import traceback
            traceback.print_exc()
            print e

def iter_parsed_feeds(only=[]):
    for feed in feed_iterator(only):
        yield (feed["name"], etree.parse(feed_pathname(feed["name"])))

def iter_search(only=[], match=[], match_any=False):
    for name, feed in iter_parsed_feeds(only):
        for detail in iter_details(feed):
            matched = [m(name, detail) for m in match]
            if match_any == False and all(matched):
                yield name, detail
            elif match_any == True and any(matched):
                yield name, detail

def match_regex(regex, case_sensitive=False, fields=["title", "description"]):
    "Match against a regex"
    if case_sensitive:
        regex = re.compile(regex, re.UNICODE)
    else:
        regex = re.compile(regex, re.UNICODE | re.IGNORECASE)
    
    def _match(name, detail):
        for field in fields:
            if regex.search(detail[field]):
                return True

    return _match

def match_glob(glob, **kwargs):
    "Match against a glob"
    regex = fnmatch.translate(glob)
    return match_regex(regex, **kwargs)

def iter_first_episodes(only=[]):
    for name, feed in iter_parsed_feeds(only):
        yield (name, item_details(feed.xpath("//item")[0]))

def print_items(detail_group, prefix="\t", cut=25):
    group = detail_group[:cut]
    for item in group:
        print prefix+item["title"].strip()
    if len(detail_group) > cut:
        print prefix+"..."
        print prefix+detail_group[-1]["title"].strip()

def pprint_download_matrix(matrix):
    for feed, items in matrix.iteritems():
        print "{0:=^80}".format(" {0} - {1} ".format(feed, len(items)))
        print_items(items)

def download(args):
    "Download Podcast Episodes"
    if args.search is not None:
        matchers = []
        if args.regex == True:
            matchers.append(match_regex(args.search))
        else:
            matchers.append(match_glob(args.search))
        _iter = iter_search(only=args.podcast, match=matchers)
    else:
        _iter = iter_first_episodes(args.podcast)
    
    matrix = defaultdict(list)
    for key, value in _iter:
        matrix[key].append(value)

    if args.silent:
        pprint_download_matrix(matrix)
    else:
        for feed, episodes in matrix.iteritems():
            for episode in episodes:
                download_podcast(feed, episode)

## REPL/eval

def eval_action(args):
    "Evaluate arguments"
    args.method(args)

def repl(args, parser):
    "Read-Eval-Print-Loop"
    eval_action(args)
    while True:
        args = raw_input("podcasts> ")
        if args.lower() == "quit": sys.exit(0)
        args = parser.parse_args(args.split()) 
        eval_action(args)

if __name__ == "__main__":

    parser = argparse.ArgumentParser(prog='podcasts')
    parser.add_argument("-i", "--interactive", "--repl", action='store_true',
                        help="run in an interactive loop.")

    subparsers = parser.add_subparsers(title="commands")

    ## Update Section

    update_ = subparsers.add_parser("update", 
                                    help="update podcast feeds.")
    update_.set_defaults(method=update)
    update_.add_argument("podcast", nargs="*",
                         help="Add podcast names to narrow the update.")

    ## Download Section

    download_ = subparsers.add_parser("download",
                                      help="download podcast episodes. If run\
                                      with no search arguments, download the most\
                                      recent podcast episode.")
    download_.set_defaults(method=download)
    download_.add_argument("podcast", nargs="*",
                           help="apply the actions to a subset of the\
                           podcasts.")
    download_.add_argument("-S", "--search", help="search for a podcast to\
                           download.", default=None)
    download_.add_argument("-E", "--regex", help="Use a regex instead of the\
                           normal shell-like globbing.", action="store_true")
    download_.add_argument("-I", "--case-sensitive", help="Run a case-senseitive\
                           search, insead of the default, case-insenseitive\
                           search.", action="store_true")
    download_.add_argument("-s", "--silent", "-d", "--dry-run",
                           dest='silent', action='store_true',
                           help="don't actually download any files, just\
                           show which file would have been downloaded.")
    download_.add_argument("-f", "--force", dest='force', action='store_true',
                           help="force downloading the the podcasts, even\
                           if they are already on disk.")

    # Actual start
    args = parser.parse_args(sys.argv[1:])
    #sys.exit(0)
    if args.interactive == True:
        repl(args, parser)
    else:
        eval_action(args)
